import torch
from typing import Literal

def compute_group_normalized_rewards(
    reward_fn,
    rollout_responses,
    repeated_ground_truths,
    group_size,
    advantage_eps,
    normalize_by_std,
    ):

    """
    Compute group normalized rewards for a set of rollout responses against ground truth responses.
    Args:
        reward_fn: A function that computes the reward given a response and ground truth.
        rollout_responses: List of responses generated by the model.
        repeated_ground_truths: List of ground truth responses, repeated to match the number of rollouts.
        group_size: int The size of each group for normalization.
        advantage_eps: float A small value to avoid division by zero when normalizing by standard deviation.
        normalize_by_std: bool Whether to normalize the advantage by the standard deviation.
    Returns:
        tuple: A tuple containing:
            - advantage: torch.Tensor The computed advantage for each response.
            - raw_rewards: torch.Tensor The raw rewards for each response.
            - metadata: dict Metadata about the rewards, including mean, std, max, and min.
    """

    raw_rewards = []

    for rollout_response, gt_response in zip(rollout_responses, repeated_ground_truths):
        curr_reward = reward_fn(rollout_response, gt_response)['reward']
        raw_rewards.append(curr_reward)
    
    # Compute mean reward for each group
    raw_rewards = torch.tensor(raw_rewards)
    rewards_per_group = raw_rewards.reshape((-1, group_size))
    mean_reward_per_group = torch.mean(rewards_per_group, dim=-1, keepdim=True)

    advantage = rewards_per_group - mean_reward_per_group

    if normalize_by_std:
        std_reward_per_group = torch.std(rewards_per_group, dim=-1, keepdim=True)

        advantage /= (std_reward_per_group + advantage_eps)
    
    advantage = advantage.flatten()

    metadata = {
        'mean': torch.mean(raw_rewards),
        'std': torch.std(raw_rewards),
        'max': torch.max(raw_rewards),
        'min': torch.min(raw_rewards),
    }

    return advantage, raw_rewards, metadata

def compute_naive_policy_gradient_loss(
    raw_rewards_or_advantages: torch.Tensor,
    policy_log_probs: torch.Tensor,
    ) -> torch.Tensor:
    """
    Compute the policy gradient loss using raw rewards or advantages.
    Args:
        raw_rewards_or_advantages: torch.Tensor The raw rewards or advantages for each response.
        policy_log_probs: torch.Tensor The log probabilities of the policy for each response.
    Returns:
        torch.Tensor: The computed policy gradient loss.
    """

    return -raw_rewards_or_advantages * policy_log_probs